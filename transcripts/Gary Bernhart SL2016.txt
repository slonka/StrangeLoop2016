   Gary Bernhardt.
   StrangeLoop 2016
   Live captioning by Norma Miller @whitecoatcapxg
   
   >> Hello! I'm not sure if someone's supposed to introduce me, but I'm just going to assume not. All right. A little bit of a preamble to this talk before I start the talk proper, this is Strangeloop, so it is much more likely that you are going to figure out what's going on in this talk than at some other conferences, so if you figure out what's going on in this talk, don't worry. That is fine. But it is not explicitly said until the end. All right. This talk is reproducibility. When I say reproducibility for the purposes of this talk immagine taking the same action in a software system, taking the same action, does the same thing every time if it's truly the same action. This of course is the main reason that computers exist, to automate, easily automatable tasks and yet it is frequently not the case that they actually behave in a reproducible way. If you've ever used make, sometimes you run make and it doesn't work and if you've used it a lot you learn that the thing you do when make fails is to run make again.
   [laughter]
   And this is a sure sign that it's not a reproducible operation. It depends on something other than the source code going into it, because the second time you ran make it was the same source code but sometimes the behavior is different and that's because object files are lying around and fundamentally it's a cache invalidation problem but that's another talk entirely. Sometimes clean isn't clean enough and so some larger projects have a secondary cleaning target. You make mrproper and that really truly cleans it and incident will I it's called mrproper because in the '90s, that was the European branding for Mr. Clean. So when clean isn't clean enough, you bring in mrproper and he cleans it up. And sometimes that doesn't work so you set your computer on fire and give up. But every step of these sort of failures is a failure to reproduce the build output. All right, so enough of the disheartening introduction, this is the only example in this talk that is not an example of things getting better. To move to some more positive stuff, let's talk about the management of DOM state in web browsers, the rendering of event hierarchy. Historically the DOM has been a big problem. Things interact in ways you didn't expect and so on and you end up with bugs. A lot of modern client-side frameworks have as one of their explicit goals to reliably manage the DOM and React is maybe the most pure example of that because it really only does this.
   If you're not familiar with React, I'll give you the sort of one-minute introduction so we know what we're talking with here. There's no model layer with React.
   There's some way in React to crease a class and this is going to be a component whose purpose is to to consume model data and render it. It takes no formal arguments but you should think of this as taking a model data as an argument. Now, every time this is called it's going to reconstruct the P tag from scratch and this is going to be important in a moment. For this talk what I'm interested in is not so much the code here, but rather the data flow that's going on, so let's look at the data flow. The input to the component is the model. This is how components are supposed to work. The component then transforms the model data into some set of DOM nodes and in fact they're what's called a virtual DOM node which is a React-specific data structure that exists simply because it's lightweight.
   React also keeps around the previous virtual DOM that was rendered last time by your component, and by comparing what it rendered last time versus what it rendered just now, it can establish a diff of the two and only the things that change. So we don't need to create it and destroy a new one, we can just update the text inside of it. And of course in real-world systems there's a lot more than just a P tag.
   Now, if I drew the DOM in a more accurate way, it would look something like this, because it is just a huge piece of shared mutable state and it's always the most expedient thing to just manipulate it directly and this topic was famously covered by Rich Hickey at this conference a few years ago when he talked about simple versus easy. It's easy to manually change the DOM more directly. So what we do with React is voluntarily confine ourselves into these blue bits. We certainly think about the virtual DOM that's produced but we don't think about the diffing process and we don't think about direct updates to the DOM itself. These render functions in React are reproducible in the sense that we mean in this talk. If you put the same data in it always produces the same DOM nodes. React relies on this.
   And this has some nice implementation benefits for React, it did do this whole diffing thing because it knows it can call your render function as often as it likes without worrying about it having side effects.
   >> But for the purposes of this talk I'm more interested in the fact that this allows a better mental model of the system you're using. They don't change the world in any way which sort of the exact opposite of the way we used to do this stuff. OK that's our first example and we're going to start right back at the beginning with a separate example about packaging and find almost exactly the same structure inside of it despite this being a completely different problem domain. Historically it has been difficult to synchronize Alice and Bob are working on the same project, Alice has been on the project 6 months longer, so she's running slightly older dependencies. This is a story that happened over and over again in the past and is less common now thanks to tools like Bundler. Bundler is a Ruby tool but most languages tip seem to have something similar to it. If you've never used Bundler here is the one-minute crash course. Here we depend on rails, 4.2. And when we Bundler it's going to look at all the Ruby packages in the world and select a subset but usually there's a subset and that is dumped into a file called gem file.lock that lists all the dependencies and transient dependencies in the system and now Alice and Bob are both definitely running action mailer, and everything is good until Bundler breaks, about that doesn't happen very often.
   So the input to the system is a set of all gems in the world and the gem file is effectively a filter function.
   
   ### here ###
   
   Now, we don't write the gem file as a function, we don't say def gem file, whatever, but if you think about it as a filter function, that model will always be correct for the way that Bundler works, it is not an approximation. Just like in React we have here a sort of pure data transaction. And a function doing the transformation and we also have an old version of the output lying around, but in this case it's the set of currently installed gems on the disk. Some of those might be lying around for manual installation or from previous bundler runs or whatever, so we need to diff the set of gems that satisfies our gem file versus the set that is currently installed and we will destructive and I think you can see this is basically the same process. And this shape shows up very often when you have systems that sort of push all the state manipulation into one part of the system, it often ends up looking like this.
   Now, once again if I drew this accurately the disk would be a huge horrible ball of state. Like in React it is attempting to touch it everywhere. And then of course inevitably you commit code that depends on it, you forgot to put in the gem file, it doesn't work on Bob's machine, now it's 2005 again and just like in React what we're doing here is voluntarily confining ourselves to these blue bits. We are choosing to think about the available gem, the set of all the gems in the world, we're thinking about the gem file that we write: This prevents all the problems of manual state management that we had before, and it is manual state management, even though you're running commands it's still the same problem and the same kind solution.
   Now, this transformation these blue bits is exactly what we mean by reproducible in this talk. If you have the same gem file, you will get out the same lock file and unlike React we don't actually do that in practice because it would just be a strange thing to do, but you can think about it that way and it will be a correct mental model and this means that it's easy to anticipate what Bundler is going to do. This once again allows us to have a better mental model of the system and prevents this class of problems that still exists to a small extent but it's probably a couple orders of magnitude less likely to have this problem than it was in 2005, which is a tremendous improvement and it's basically a free lunch, which do exist, by the way. So that's example 2, and example 3 is not going to have this same structure. It's everyone's favorite piece of software in the world, Git. But I want to clarify that everything in this talk is not about the git UI. The UI, yes is a nightmare, but the data model is actually fantastic. If you pull out your laptop right now and go into git repo and create a file that context foo/new line. The word blob, because this is the contents of a file which is called a blob. A No. 4 because there are 4 bytes in it. 0, and then the actual text of the file. Git is then going to hash this and get had been 527C. Which is going to be the key under which this is stored. This will happen regardless of your user name or the name of the file or what time it is, or what the random generator seed is or what's on the network of the disk and so on. This is a purely reproducible operation that will happen on every machine in the same way. The same thing is true for trees of files. If you create the same nest of files. If you commit that tree of files with the same user name, the same time stamp and so on you will get exactly the same hash on different machines that do not communicate with each other. This is another example of reproducible behavior because if you put in the same stuff into git, you get the same git repo out even on machines that don't communicate and this is central to the way that git works for a number of reasons at least most easily to explain this gives us free integrity checks. So when you pull the object out you rehash it, make sure the hash matches its address and if it doesn't, you complain very loudly and refuse to continue, which is what Git will do, when your file system corrupt itself, which sometimes it does. In addition it gives us free deduplication. If two pieces of data are the same, they have the same hash, so they're stored under the same key so there's no duplication.
   This is a massive improvement over, for example, mur curial. In git repo it's impossible to have that problem. But anyway, I just love talking about how awesome git repo state is. In this case we are talking about the mental model of git repo is better than other control systems, because all these little pieces are defined in very clean ways, very simple ways and this is why for example experienced git repo users can pretype the next command before the next one finishes or a fancier way to say that is they can close their eyes and use git and that means their mental model must be so good that they can -- it's because all these pieces of git are defined in these simple ways with one input and one output and so on and it has a nice simple graph and all that stuff.
   OK, so that's a third example and now I can tell you what this talk is actually about. This -- all of these three systems are so easy to understand and so widely loved, I think, because they expose pure functions in their mental models, but that does not mean they are examples of functional programming. In fact, all three of these are written in highly nonfunctional languages. None of them is even written in a functional style, at least in any kind of rigorous way, but they expose these fundamental mental models and that's why they are so easy to understand and that's why we love them so much.
   The react rend :
   >> Bundler it's a little bit less clear. I think most Bundler users do not think explicitly about the fact that there is a transformation to all gems to the gem file.lock. They know how the system works and this is what it's doing. By the way, Bundler does not pull down the set of all available gems, but if you think about it that way, it is absolutely a correct mental model and it will never go wrong.
   In case of git, all of those are pure functional transformations, even though git is a giant mess of C code that has lots of stuff in it. And the central claim of this talk is this is why I think people who use these tools love them and especially why, someone who, for example, ha does a lot of JQuery and learn React, feels a sort of burden lifted from them because now they don't have to have this nasty problem of shared mutable state. The same thing is true of a Ruby programmer who learn Bundler for the first time, which of course most people learn it early now, but when it was introduced it was a tremendous improvement, and I think the same thing is true for anyone who has used a version control system very well and then learn hour git returns early. It's actually refreshing to understand what's actually going on inside of it.
   Now, when I do talks I like to anticipate objections that I might come up with if I were watching the talk so I want to address the possible objection that maybe it's obvious that systems should be designed in this way. For example if you started programming after 2005 and its design may seem obvious to and git's design is obvious once it's been explained to you. But is it obvious when it has not been explained to you, that's the more important question for making new things that are as nice to use as git. Well, let's look at the timeline, the history of version control because out of these three examples it has the longest history. I don't know the ancient history of version control but the first thing I do know about first hand is RCS, which was designed in 1982, and it could version only a single file at a time, so that wasn't so good. Eight years later somebody took some duct tape and duct taped together ... So there was no concept of an atomic commit that you could go back to. For that you have to wait an entire decade until 2000, but of course subversion was centralized and didn't have a content address storage system and so on. That all happens in 2005, with git, and also some of that stuff happens with mercurial and other systems around this time. Now, that was 23 years, so given how obvious git seems in hindsight, why did it take 23 years to get there? Well, let's enumerate some possibilities. Maybe we were missing some technology, like a sufficiently good hash function but of course that's not the case, because MD5 existed in 92. Maybe, computers were too slow to run git back then. And certainly that was probably true in 1982, because there is a lot of hashing involved, but by the time we had, let's say a Pentium 2 Class C PU in '97, that would have been sufficient, keep in mind also we're talking about MD5 on smaller amounts of files, so it would have been fine even though it's only a 10th as fast as current computers.
   Maybe it is extremely difficult to implement git, and this one is the most interesting to me, because in the course of preparing this talk I learned that git was self-hosting four days after Torvald is s starting writing git: If you check out the source code of git, git.git right now in your laptop and go to the very first version, initial version of git, and the date on this is Thursday, April 7th, 2005, which is four days after he began implementing it, so the commit is still there, he just dumped the whole thing out of his brain into whatever weird version of E max he uses that I forget the name of, in four days, so it's not difficult to implement git. It does not require a lot of CPU. It does not require primitives that were not available at the time, it simply is the case that it actually is highly nonobvious despite a fact that git state model is so obvious. And this is the thing that is tricky about recognizing or about appreciating how difficult it is to build a tool that is nice to use, because in hindsight it looks so obvious, but you spend decades not doing it. If I had actually named this talk in a way that explained what it was about, it would be called, reproducibility leads to good mental models which lead to tools we love whose designs are highly nonobvious. That would be a much more academic title. I like single words because it's just an affectation, but this is my sort of summary of this talk. This property of software systems allows us to understand what is actually going on inside the thing, which leads to us actually liking to use the thing as programmers and maybe this is not a good model for user-facing software, but for software development tells I think it is a fantastic default for implementation.
   Now, I do have two sort of concrete calls to action as a result of this talk and they're very different. The first of them is if you're building software development tools, or if you want to, please build them in this way. Look for large data transformations that you can sort of present as part of the mental model of the system, so that the programmer can think about a large piece of data going into a transformation, another piece of data coming out. Nothing changes, it just gets transformed to one form to another, and that will increase the probability that people will like to use your tool and also decrease the probability that I will be very angry when I use your tool and it's confusing to me.
   The second sort of call to action is, convince people of the value of functional program in this way. Because when we do things like -- we introduce functional programming by saying, OK, Haskell has this dot function and you can compose map with itself and now you have a 2D map. OK, great, maybe that's cool. Maybe someone thinks that's neat, but it doesn't tell them why they should possibly ever care about it if they're interested in building software, but if you can tell them, hey, remember how bad it was to write ad hoc and how much of a relief it was to use React as a system, that's because we went to a system that had more pure functions in it and wouldn't it be nice if there were more languages that used that. So that is reproducibility. I have only one more thing to mention that to destroy all software is resurrected. I am publishing some very quiet clapping. That's about what it deserves, I mean. I'm sorry, I wasn't fishing for accolades there. Anyway, I'm publishing a series that is basically the equivalent of a theoretical computer science course in an undergraduate curriculum, but approached using only just code and everyday programming languages. No mathematical and with that, I thank you all for coming and listening.
   [applause]
   >> I think I problem have a tremendous amount of time because I always make my talks short so I'll take maybe like three questions which is about what I have the emotional capacity for after doing a talk, so if anyone has questions or questions that are actually comments, I will also accept those.
   [laughter]
   >>
   >> One right.
   >> Yeah, go for it.
   >> So I'm thinking about the learning trajectory of going ... and I [inaudible] I'm curious if you think that someone in the same way that people used git in the beginning can someone appreciate some of these more like functional mental models of tools without going through the pain of doing something [inaudible]
   >> That was such a good question, I'll try to summarize it. So for people who have come into -- who have become programmers in a world where React is already a thing, can they appreciate the value of it without having experienced the pain, I think it maybe a short -- the best I can summarize that. There are definitely a lot of programmers who think that you should experience that pain and this has always been true.
   [laughter]
   So for example, I mean I'm thinking about like, let's say  -- let's say 20 years ago when java was becoming big and everyone was up in arms because now programmers aren't going to have to go through C, they're not going to know how a computer works, and that's bad. Which in a sense that is bad in the sense that if you know how a computer works, that's an additional skill that you have. Julia Evans gave a fantastic talk yesterday about paying attention to how a computer works, although she didn't phrase it that way. But there's a weird psychological thing where once you know the easy way, the hard way is only something you make fun of, I think. And it goes the other way, as well, if you know how computers work, then you tend to make fun of people, as well. So I don't know, I think it is very valuable. You know what, I should say I do know people who went to boot camps and they then they went back to learn C, and I think they valued it. So I would encourage anybody who has grown up in the world of git go back and just use CS every day. Or RCS even better if it even compiles anymore, probably doesn't. Nobody uses it and definitely using C is it very valuable. I just rambled forever for that question, so thank you all very much for coming.
   [applause]
   
   ... ... ... ... ... good afternoon everyone. Hope everybody had a good day, saw some good stuff, knitting machines, things of that nature? I just wanted to say thanks to a lot of people. I don't have any announcements other than that, but there are tons of people that go into making this a great event so I think it's apropos to mention them here at the end before we close out the event. So I want to give a big round of applause to all the sponsors.
   [applause]
   This morning I thanked awesome of the other organizers, so a big thanks to them.
   [applause]
   [cheers]
   I want to give a big thanks to all the speakers, obviously they're talking about all the great stuff that we're here to see.
   [applause]
   [cheers]
   I want to thank Norma, who's been doing the amazing captioning down here all day for three days. 
   (Mad cheers and applause).
   [laughter]
   >> She's -- she accepts [Mastercard, Visa...] 
   [laughter]
   She accepts thanks in the forms of margaritas. So I hear.
   [applause]
   [laughter]
   >> I want to thank the video crew, Ian's down here we've got several guys running the video cameras and everything. They do a fantastic job, big thanks to them.
   >> If you hadn't noticed, all the talks from yesterday are online already and we'll have you, you know, today's talks up within the next day or two and I know that they'll be working on Elm Conf and PWL by early next week. All of those will be online. So check those out. And then the last thing I wanted to do was thank the Peabody staff here. The AV staff is amazing. We always have amazing support for video and audio and the back of house staff, Adrian is the main guy there but all these people are what makes us all seem seamless and flawless to all of us and so huge thanks to the back of house staff and also to the front of house staff, the people who have been helping you with the catering and the ushers and people at the door and all those people.
   >> So.
   [applause]
   [cheers]
   >> They're always fantastic and thanks to George at the dock, who is my buddy. Helps me get all the packages and all the things that I need, so he's been here for years and is always a joy to come back to. So I think that's it. Thank you to everybody who helped make this great. And we have one final great talk for you today. So our final keynote is Leigh Honeywell from Slack is she is now the manager of security response, so Leigh Honeywell.
   [applause]
   [cheers]
   >> Hey, everyone I'm so excited to be here. I've heard so many great things about Strangeloop over the years and it's just such an honor to get to talk to you about some of the things I've learned about security over the years so I'm going to be talking about building secure cultures. My slides are online already so you don't have to take notes. There's lots of creative commons attributions in the notes as well as links to various resources owe check them out.
   Also, quick content note, I am going to talk about plane crashes and like pretty minor, so how they're like investigated and solved so if you're not a fan of plane crashes just a heads up. So me, I work at Slack on the security response team there I'm managing it. I used to work at Heroku, Microsoft, Symantec, I paint and I'm sometimes a cranky feminist. I was raised by a feral pack of Canadian lawyers and I think lawyers are actually important for society, I think it's actually security people that ought to be pop gizing, it's also just partly being Canadian, so sorry, that's how you know someone's Canadian, you bump into them and it if they apologize to you, they're Canadian.
   So yes get told to move fast and break things. This is one of the mantras of Silicon Valley. But then this happens. This is he tried to report a security bug to Facebook through these usually excellent bug bounty program, they misinterpreted the result, turned down the bug, he posted on zuck's wall and then we get stuff like this. Heartbeat is a fascinating study of the security response in the open source world. A researcher at Google found the bug, went through various property channels to report it and it got shared with the Linux, a few cloud vendors, when you read through a detailed timeline of the event you start to see where the information starts to move around and there's starting to be just enough research out there that other researchers go poking and some Finnish researchers found the same bug and then the cat is out of the bag and the embargo actually broke, so it was released a Monday morning instead of a Tuesday. But I want to go back even further. This is a map of the world on July 15th, 201. This is the code red worm. It exploded a vulnerable in MS that had been fixed. The reason I know those numbers, ten years later I worked on that team, the Microsoft security response center, I handled security vulnerabilities, the thing that was pretty school there, we had this whole set of policies and practices in place where we could work with external people to take in security vulnerability information, we had all this telemetry where if someone was using malware in the wild and it would cause crashes, there's a great story in the notes, I'm not going to go into in detail that really, you would find these bugs that were being actively exploited, it would show up in telemetry and you'd be able to reverse engineer what was the vulner ability. It was this methodology cease less every second Tuesday of the month and every once in a while a second time in the month for emergency patches process of updating a billion computers around the world. So if you were running Windows in 2012, I rebooted your computer and sorry. So in that job I got to work internally within Microsoft, with developers, testers, executives, Pr, everybody, coordinating shipping patches in Windows, shipping patches in Office and working with the external researchers who were often reporting bugs to us and in the best cases we would take the lessons learned from those bugs that were reported to us, cycle them back into the development process and build more secure software in the future and at Slack we do something pretty similar. We have a bug bounty program where appointly $177,000 in bountis paid out. We've had hundreds of researchers around the world report bugs to us. **.
   But still an after the fact kind of thing. So it makes me wonder, like what does it mean to build secure software before you're shipping it, right? Bounties, security response, all of this stuff is the cat's already out of the bag, your site's already owned, your desktop software that you shipped already has a bug in it. What can we do before we ship that code to make things better? So some of the things that looks like in an organization that has a healthy security culture is you have developers reaching out to the security team when they're stuck or unsure about the implementation of a feature. You have developers finding bugs in each other's code, security bugs during the code review process, you have enough tooling, testing in place that people feel like they're protected from small errors or regressions. And one of the big ones, one of the big like organizational smells of a healthy security culture is you have people saying, like fessing up when they make a mistake about security, reaching out proceed actively saying, oh, hey, I posted that code in the wrong window, I deleted it and cycled it all right, but I just wanted to let you know, also don't put your Slack token on GitHub, it makes me really sad. Has anyone had a Slack token that's been nuked that was posted on GitHub? No, OK, we nuclear them proactively. Yes? Yay! So we proactively go out and nuclear those token.
   What we're talking about here is complexity. The software that any of you know about is unimaginably or knowingly complex. We could blame the weird janky export regulation that people had to do, but in any sufficiently large codebase you're going to have those weird dark corners.
   You're going to make mistakes, we're human, we're humans, we make mistakes, you're going to introduce security bugs. One model that I like to think about in thinking about security bugs and complex systems, it's called the Swiss cheese model of accident. He talks about the latent hazards in the system, the holes in the cheese. In security, and in testing we call this bug-chaining. I first learned about this model from the book the digital doctor, hope, hype, and harm at the dawn of medicine's computer page which I particularly like because we are not just at the dawn of medicine's computer age we're really at the dawn of medicine's computer security age, too. We're all beginners at this.
   So what can we do about this? Right? We have these complex systems, they're error-prone, they're humans, they make errors, any different part of your computer system can fail. So how do we get to this solid block of cheese that errors cannot pass through? In thinking about your system you've got your components, tooling, you've got the humans involved, underlying infrastructure, cloud, somebody else's computer and each one of those is going to have holes so all that we can hope to do is make them smaller, make sure there are fewer of them and make sure they don't line up. So here are things that anyone here in the audience can do. The first thing is to go looking. As a developer there's a lot that you can do as an individual to make you learn who make your code better from a security perspective. Whether it's reading up on the latest classes of threat. Doing some at that time static analysis, reading some books or getting some training in security. But fundamentally, though it's a mind set thing. You're putting yourself in the shoes of ...
   So back when vista was under development, everybody's favorite operating system, the powers that be paid a bunch of researchers to do a full code audit and they actually ended up slipping the release of vista, because they found so many bugs. In 2011 I saw a talk given by one of these testers. Her NDA had expired and she was like, I'm going to tell all, I'm going to spill the beans, it was great. So the things that she was able to develop in doing this massive massive scale code audit there was no way that even with the pretty large security groups they had they couldn't go through every single line of code so they interviewed teams and they developed a really strong sense of security smells, so the security smells she talked about were things like people's body language, whether they seemed calm or anxious as they were talking to security people. Confidence or lack of confidence in describing the functioning of their component and so as they worked with these different developers, P MS, different folks, around the wipedos organization, they were able to say, oh, this is code, there be Dragons here. And the effectiveness was shown by just how many bugs they found. So as a self-awareness thing when you're writing code, paying attention to that gut feeling, paying attention to like this component is keeping me up at night, not ignoring that. And when you have that feeling, asking for help, whether it's from your security team, appear doing some additional research yourself, but following those hunches, developing that sense of smell for your own code and if you reach out for help within your organization and you get shot down, that's a pretty useful signal, too, maybe not a great one, but yeah, so the other thing that I love to tell people about if they haven't heard about before, is capture the flag. So there's all these hacking tournaments that people put on and I'll have some links later. It's basically jeopardy board here's the 200-point web app hacking challenge and it's a great time constrained way of learning new skills. CTF.org has a great directory of them. To get a team going, all you need is some sort of chatroom, agnostic to what kind and a Google doc. I will note that security is maybe like 10 years behind the rest of the world when it comes to having welcoming environments and often if the event has an IRC channel it will be kind of foul. It's still worth it to me, but heads up. And then, beyond just what individual things people can do, I thin it's really important to think about organizational processes. *.
   So more stuff I learned at Microsoft, they have this security development life cycle. It's this sophisticated, 100-plus procedure thing, compliance thing that people do. Despite its complexity, there's a lot of really interesting stuff that people can learn from it. It's all creative commons licensed and worth checking out. But it's gotten me thinking a lot, like this is great when you're shipping Office every two years, but it's less feasible for those of us in the room who ship code every day or multiple times a day. So the big thing -- the Mc given I've been chasing for a couple of years is what does this look like when you're shipping code multiple times a day? So I've been thinking, what's a minimum viable SDL? Going beyond this big process to what anyone can implement in a smaller organization? And I've come up with these four things. A risk assessment up front, getting people to think about what the security posture of a particular feature they're working on is, doing some threat modeling. It just needs to be I'm thinking through the data flows of my project up front. A checklist. And theres' a ton of different resources you can use to build those checklists on your own, whether it's the Mozilla secure coding checklist, the OS resources or Microsoft's own SDL, I can taking the parts from that that make sense to you.
   And then security analysis integrated getting those tools in place.
   So here's what it looks like at Slack. We built a self-service SDL that any team can go in and create an SDL project for a new feature that they're working on. It's been a team effort. Our head of product security, it's my friend Ari and he couldn't be here, but this is -- this is as much his work as it is mine.
   But you start talking about process, life cycles, at small organizations and people freak out. Also because, this is the section where I'm talking about Slack stuff there's a lot more eemojis coming up u just heads up. People are like, what is this process? I don't want process, you're adding friction, you're slowing me down, so you get this conflict between security and dev, but it doesn't need to be like that.
   [laughter]
   In fact, I want it to be more like this.
   [laughter]
   Some people may recognize but yeah, I want security to be like that, is that so much to ask? What does that actually look like? *.
   For us it's the initial risk assessment. This is something that the Microsoft team pioneered and that's instead of the two weeks before you're shipping you're like I have to do all the security stuff, it's getting people thinking about it at the start of the project and throughout the process rather than just on that last crunch.
   So we have this little quiz that you do, it's only like 6 questions long, and various answers will dump you immediately moo high risk or you can put yourself into high risk and say yes, I'm writing SQL injection. So the questions are pretty straightforward and once you've got that risk rating it's going to go through the component survey, so the component survey allows people to opt into the things that they're doing, so if there's no mobile aspect, they uncheck the mobile part of the checklist. So the checklist looks like this, super-straightforward, and you can uncheck -- everything is checked by default at the top level and you can check the specific things, the subtopics that you need, and you can uncheck the top-level things you don't need, but we encourage people to err on the side of if you're not sure about it, just leave it checked because you can move it into the I don't need to do this column later and nobody gets mad or anything.
   So before I get to checklist, this is the part where I talk about plane crashes, so any fans of the air crash investigation show in the room? Oh, yeah, I'm glad. I'm glad I'm not the only one. So checklists are a huge part of aviation safety. And if you want to see really -- the thing that is so great about the air crash investigation show is there's terrible things that happen and then competent humans show up and they investigate them and they make processes so they never happen again, and that's why, like, air travel is safer than any other kind of travel, pretty much. So most aviation accidents result from human error and one of those root causes is often a failure to comply with checklists, but thee checklists also prevent a ton of accidents and so when we were thinking about how we were going to design this process, design this checklist, we wanted to learn from the human factor stuff of aviation safety for like, why do people avoid checklists, what could we adapt from that? So my colleague read the entire 300-page report in the use and design of checklists and we went through that and we're like, what are the parts of this that matter to security?
   So there were -- there was like some fundamental stuff of where checklists fail. It's failing to use the checklists, failing to verify the settings visually, being interrupted is a big one. So some of the things that we incorporated into hour own process, were oversight of completion of the process from the security team, the security team being involved in being tagged in at different parts of the process, making the tasks as simple as possible, and I'll go over this a little bit more, but their affirmative statements, I'm not -- yeah, affirmative statements, rather than I'm not doing the wrong thing, I am doing the right thing.
   So and then we also have the feedback cycle from the bug bounty when our checklists fail, we usually hear about it pretty quickly, which is nice and if you don't feel like reading a 300-page FAA document I definitely recommend the checklist manifesto for a slightly less dry version of a lot of the same information.
   So here's what our checklists actually look like. First of all once you've gone through the generator thing, it starts generating some Trello boards, takes a little while, and then you get a board of all of your different checklists for the different categories of security things that you need to deal with.
   Then there's a checklist for adding your Trello board to Slack. It's not the most elegant part of the process, but here's what some of the checklists actually look like, so we've got regular expressions and then we've got alert and warning messages and the alert and warning messages stuff is actually taken from the Microsoft SDL because their guidance is really good on that one and relevant to what we do and then we can have a visual overview of what's been checked off and what hasn't and most importantly, because we've connected it to Slack, we have a nice little SDL bot that tells people in your feature channel, you've connected it to the feature channel. We have a lot of channels in general. Like, (stage whisper) a lot, it's kind of bad. So here you have a couple of examples of what this looks like, so we have a checklist item and then Mary is like, hey, Josh, we're not ready to check that one off, can you leave it unchecked for now, and we'll come back to it. Or Josh is verifying with the rest of the team that this particular item has been correctly, correctly dealt with, we've disabled DTT parsing, yeah, I can check that one off. So you're bringing the security stuff to where people are actually working.
   And the reason this matters is that it moves away from that one poor tester who has to do the 120-item checklist two weeks before the launch to a collaborative enterprise, to a collaborative effort of people working together to make sure that you're shipping secure code and it also means that as teams are working on their on, getting through a particular set of checklists, if they're ever like not sure how to do things, they just tag us in, so here the team is asking Maria for help with a particular UX decision that has an impact on security. Maria brings in, hey, here's an example of how a third party does it. That would work well in this case, I think. And then the team is like, oh, yeah, that totally fits our purposes and Maria just pops out doesn't need to be in that channel all day.
   And then we have the bug bounty, people are able to file bugs themselves internally, the rules are just JSON files that live in our GitHub, so we can accept PRs, often grammar edits from the rest of the organization and we are able to move the secure development process quickly, not just move our general software development quickly. So that's what we've done as an organization, but then there's also the question of like what can -- what are the cultural factors at work? So there's a number of things in terms of the way we create cultures that affect the security of the software that we produce, and holding ourselves, those among us who are leaders in hour organizations, which I suspect is many of you, holding yourselves accountable for the things we create and the impact on culture it has, it's pretty standard blameless culture of for fin fans of Etsy's code is craft blog. People don't want to be the one who ships the code that has XSS in it, so we need to be able to figure out, what resources do we need to get people so that they can ship secure code, whether it's time, training, or external expertise.
   So recurs center social rules. Security teams, super-often guilty. Of honestly, all of them, but let's talk about feigned surprise, right? How didn't you know that that would cause XSS, like duh!? Who's heard something like that from their security team, right? Yeah, if there's one thing that I tell here security people all the time, I like hold that sign up in their faces and say, like don't do that, it's not necessary, that that piece of feigning surprise, like it just as the rules say it has no social or educational benefit, when people feign surprise it's to make themselves feel better and making other people feel worse. It's about making other people feel smaller and that's not how you write secure software. Fundamentally writing secure software requires a level of emotional safety. I've often had the experience of trying to talk to a security person feeling like trying to pet that cat and as I say that as a security person. So there's this idea of emotional labor, that it is actually work to give gentle feedback, to be kind in the feedback that you're giving to other people. It is also emotional labor to be receiving feedback, to be open to hearing that maybe your baby is ugly. And but this isn't just like pie in the sky like let's all get along and write secure software. It's actually supported by some of the social science research into code quality, and when this comes down to it, writing secure code is writing quality code and writing quality code is writing secure code. So if we are kind in the feed back that we give, we are building a culture in which it is safe to say I don't know what I'm doing here and when people are able to say I don't know what I'm doing, they get help and then they write code that is better and that includes security. Yeah. Nobody likes being -- nobody likes their baby being called ugly, but we need to reestablish that trust as security people, because people need to be able to hear if their baby's car seat isn't plugged in or the seatbelt is Frayed as otherwise disaster, so being able to create a culture are where you don't mind saying I don't know how to create XSS. So coming back to the digital doctor, I really like this quote, because it cuts to the core of why we need to be able to have trust in each other to do security. We need to be able to talk compassionately about the ugly parts of our code, if we have any hope whatsoever of it being secure.
   And I talked a lot faster than I expected, but again, there are slides available, and, yeah, thank you very much for listening to me talk about secure software.
   [applause]
   >>
   >> So it being the last talk of the day, we're not going to do questions up on stage, but if you want to talk to me about this stuff, please feel free to come find me afterwards. Yeah.
   [applause]
   >>
   >> Thanks, everyone and that's a wrap. I hope you all had a great time at Strangeloop and you have a great time getting home, and thanks so much for coming.
   We'll be back here sept 28th through 30th next year, so keep an eye out ... ... Office.
    
